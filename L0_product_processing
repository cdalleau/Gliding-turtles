########################################################################################################
# Process to generate a NetCDF file with raw data (L0 product) using location data
# Project: SOCIB Seaturtle
# Author: Chlo√© Dalleau
# Co-author: David March
# Date: 08/08/2016
#
# Description:
# This script is the first step of a global processing about the analysis of the turtles' trajectory.
# The following steps create a NetCDF file using the raw data of the turtle:
# - (1) Data import from CSV files or from Wildlife Computers (WC) data portal: time, latitude (Argos/GPS),
#       longitude (Argos/GPS), location quality, number of satellites available from GPS, residual from GPS, temperature, depth, battery.
#       Please note that a variable cannot be chosen several times from different csv files (except time and tagid).
# - (2) CSV file (generated by DAP software from Wildlife Computers) merging in a NetCDF file
# 
# The process uses two additional CSV files :
#       * meta_data: a file with all the information about the turtles such as: name, id, date deployment ...
#       * allparameter: list of the variables or global attributes which will be created in the NetCDF file. This file allows an
#       automation of the creation of the NetCDF file. It contains:
#           - the name of the variable
#           - the long name
#           - the unit
#           - the details
#           - the NetCDF variable type (NC_DOUBLE,NC_CHAR,NC_INT ...)
#           - specify if the variable is used in an another process (here : L0product, L1product and L2product)
#           - if the type is NC_CHAR, specify the dimension
#           - the dimension used in the NetCDF file in the different processes (here : dimL0product, dimL1product and dimL2product).
#       Such as :
# var_name,long_name,units,details,value_type,L0product,L1product,L2product,dimCHAR,dimL0product,dimL1product,dimL2product
# time,time,seconds since 1970-01-01 00:00:00,,NC_INT,x,,,,time,,
# source_loc,source of the location,,,NC_CHAR,x,x,,max_string_32,time,time,
# NC_GLOBAL,otherTypeLoc,,,NC_CHAR,x,,,,,,
#
#
# WARNING:
# - There are differences in date formant between CSV files (eg. Locations vs. Series) and tutles (eg. processed by WC Data portal vs DAP).
#   Main problem is the name of the month (eg. 'ago' or 'aug' for Spanish or English, respectively). Check this in Step 2. Suggestion: process all tracks in English (check manual for DAP)
# - We remove duplicates in time for each CSV file processed. Some duplications in time may have different data. We delete one of them. In future versions, check if we can use any quality control criteria.
# - Current script works for tags with temperature and pressure sensors (ie. Series.csv) and with battery information (Status.csv).
#
#
# Sources:
# - Michna, P. & Milton Woods. RNetCDF - A Package for Reading and Writing NetCDF Datasets. (2013).
# - Michna, P. & Milton Woods. Package 'RNetCDF'. (2016).
# - units: http://www.unidata.ucar.edu/software/udunits/udunits-2.2.20/udunits/udunits2-derived.xml
# - standard names: http://cfconventions.org/Data/cf-standard-names/34/build/cf-standard-name-table.html
########################################################################################################

### Remove the previous data
rm(list=ls())

### Import libraries
library(RNetCDF)
library(lubridate)
library(curl)
#library(wcUtils)  # used in step 1, taken from https://github.com/jmlondon/wcUtils


### Set the turtle ID (Ptt) and if the data should be downloaded from the WC data portal 
tag_id <-  151933 # modify the tag_id according to your turtle (e.g. 138120 151933)
download_from_WCDataPortal <- "no" # if "yes", step 1 will be processed
lan.setlocale = "English" # Warning : parse_date_time work with "Spanish" for the turtle 138120 and "English" for the turtle 151933

############################### Step 1: creation of the CSV file from WC Data Portal ##################

if (download_from_WCDataPortal == "yes") {
  
      # Define location and file with auth keys
      keyfile = "keyfile.json"
      
      # User ID
      owner = "55beb2152484c8f51dfea90a"  # this ID was obtained after inspecting the Data Portal with Chrome developer tools
      
      
      # Folder to download data
      path = paste("data/rawinput/",tag_id,"/.", sep= "")
      
      # Argos Platform ID
      ptt = tag_id
      
      
      
      ## Define function to dowload data
      
      wcGetPttData <- function (keyfile, owner, path, ptt){
        
        # Get deployment information
        params = paste("action=get_deployments&owner_id=", owner, sep="")
        wcpost <- wcPOST(keyfile= keyfile, params = params)
        
        # Get PPT ID
        id <- wcGetPttID(wcpost, ptt = 	ptt)$ids
        
        # Download and extract ZIP files to obtain CSV files
        zipPath <- wcGetZip(id, keyfile = keyfile)  # download zip file in a temporal folder
        unzip(zipPath, exdir = path)  # extract all .CSV files from ZIP
        # file.copy(from = file,to = newfile,overwrite = TRUE) #  copy the zip in rawarchive
      }
      
      
      wcGetPttData(keyfile, owner, path, ptt)


}



###################################### Step 2: Import data ###########################################

### Meta data import
meta_data <- "data/turtles_metadata.csv"
meta_data <- read.csv(meta_data, sep=",", dec=".", header=TRUE, fill=TRUE)
colnames(meta_data)<-c("argosid", "name", "dateDeployment","refMaxSeaTemp","refMinSeaTemp","refMaxDepth","refMinDepth", "title", "author", "publisher","fileVersion","otherTypeLoc")
meta_data$argosid<-as.character(meta_data$argosid)
meta_data$name<-as.character(meta_data$name)
meta_data$dateDeployment <- as.POSIXct(meta_data$dateDeployment, "%Y-%m-%d %H:%M:%S", tz="GMT")
meta_data$refMaxSeaTemp<-as.numeric(as.character(meta_data$refMaxSeaTemp))
meta_data$refMinSeaTemp<-as.numeric(as.character(meta_data$refMinSeaTemp))
meta_data$refMaxDepth<-as.numeric(as.character(meta_data$refMaxDepth))
meta_data$refMinDepth<-as.numeric(as.character(meta_data$refMinDepth))
meta_data$title<-as.character(meta_data$title)
meta_data$author<-as.character(meta_data$author)
meta_data$publisher<-as.character(meta_data$publisher)
meta_data$fileVersion<-as.character(meta_data$fileVersion)
meta_data$otherTypeLoc<-as.character(meta_data$otherTypeLoc)

### Meta data selection using the turtle ID
select_data <- meta_data[which((meta_data$argosid == tag_id) == "TRUE"),]

### Select correct CSV file
## Locations : location of the turtle and quality location from Argos
## Series : temperature and depth and their errors
## Status :  battery voltage just prior to transmission
## 1-FastGPS :  location of the turtle and quality location from GPS
if (select_data$otherTypeLoc == "GPS") {
  name_file <- c("Locations","Status","Series", "1-FastGPS") # Warning : the order is important
  name_data <- c("loc_data","status_data","series_data","gps_data") # Warning : the order is important
} else {
  name_file <- c("Locations","Status","Series")
  name_data <- c("loc_data","status_data","series_data")
}
level <- "L0"

### Data import from CSV files using either WC data portal or the file version.
dirfile <- dir(path=paste("data/rawinput/",tag_id,sep=""), pattern="*.csv$") # select all the names of csv files contained in the path chosen
file <- c()
# Warning  locations !=  Locations
# Warning let ".csv", if no select Series and SeriesRange
for (i in 1:length(name_file)) file[i] <- dirfile[grep(paste( tag_id,"-",name_file[i],".csv", sep = ""),dirfile)] # select the names of the csv files chosen in "name_file"
for (i in 1:length(file)){
  if (name_data[i] == "gps_data"){
    ff <- paste("data/rawinput/",tag_id,"/",file[i],sep="")
    data <- read.csv(ff, sep=",", dec=".", header=TRUE, skip = 3)
    assign(name_data[i],data)
    rm("data")
  } else {
    ff <- paste("data/rawinput/",tag_id,"/",file[i],sep="")
    data <- read.csv(ff, sep=",", dec=".", header=TRUE)
    assign(name_data[i],data)
    rm("data")
  }

}

### Select interesting variables
## Locations.csv
loc_data <- loc_data[,c("DeployID","Date","Quality","Latitude","Longitude")]
colnames(loc_data)<- c("tagid", "time", "lc", "lat", "lon")
loc_data$tagid<-as.numeric(as.character(loc_data$tagid))
loc_data$time <- parse_date_time(loc_data$time, c("HMS dbY", "Ymd HMS"), locale=Sys.setlocale("LC_TIME", "English"), tz="GMT")
loc_data$lc<-as.character(loc_data$lc) 
loc_data$lat<-as.numeric(as.character(loc_data$lat))
loc_data$lon<-as.numeric(as.character(loc_data$lon))
## Series.csv
series_data$Date <- parse_date_time(paste(series_data$Time, series_data$Day, sep=" "), c("HMS dbY", "Ymd HMS"), locale=Sys.setlocale("LC_TIME", lan.setlocale), tz="GMT")
series_data <- series_data[,c("DeployID","Date","Temperature","TRange","Depth","DRange")]
colnames(series_data) <- c("tagid", "time", "temp", "errorT", "depth", "errorD")
series_data$tagid <- as.numeric(as.character(series_data$tagid))
series_data$temp <- as.numeric(as.character(series_data$temp))
series_data$errorT <- as.numeric(as.character(series_data$errorT))
series_data$depth <- as.numeric(as.character(series_data$depth))
series_data$errorD <- as.numeric(as.character(series_data$errorD))
## Status.csv
status_data$Date <- status_data$Received 
status_data <- status_data[,c("DeployID","Date","BattVoltage")]
colnames(status_data) <- c("tagid", "time", "batt")
status_data$tagid <- as.numeric(as.character(status_data$tagid))
status_data$time <- parse_date_time(status_data$time, c("HMS dbY", "Ymd HMS"), locale=Sys.setlocale("LC_TIME", "English"), tz="GMT")
status_data$batt <- as.numeric(as.character(status_data$batt))
## GPS.csv
if (select_data$otherTypeLoc == "GPS") {
  gps_data$Date <- parse_date_time(paste(gps_data$Time, gps_data$Day, sep=" "), c("HMS dbY", "Ymd HMS"), locale=Sys.setlocale("LC_TIME", "English"), tz="GMT")
  gps_data <- gps_data[,c("Name","Date","Satellites","Residual","Latitude","Longitude")]
  colnames(gps_data) <- c("tagid", "time", "satellites","residual", "lat_gps", "lon_gps")
  gps_data$tagid <- as.numeric(as.character(gps_data$tagid))
  gps_data$time <- parse_date_time(gps_data$time, c("HMS dbY", "Ymd HMS"), locale=Sys.setlocale("LC_TIME", "English"), tz="GMT")
  gps_data$satellites <- as.numeric(as.character(gps_data$satellites))
  gps_data$residual <- as.numeric(as.character(gps_data$residual))
  gps_data$lat_gps <- as.numeric(as.character(gps_data$lat_gps))
  gps_data$lon_gps <- as.numeric(as.character(gps_data$lon_gps))
}



### Removing locations from testing environments before releasing
loc_data <- loc_data[which((loc_data$time > select_data$dateDeployment) == "TRUE"),]
series_data <- series_data[which((series_data$time > select_data$dateDeployment) == "TRUE"),]
status_data <- status_data[which((status_data$time > select_data$dateDeployment) == "TRUE"),]
if (select_data$otherTypeLoc == "GPS") gps_data <- gps_data[which((gps_data$time > select_data$dateDeployment) == "TRUE"),]

### Remove duplicate time measurements
if (length(which(duplicated(loc_data$time)))!=0) loc_data <- loc_data[-which(duplicated(loc_data$time)),]
if (length(which(duplicated(series_data$time)))!=0) series_data <- series_data[-which(duplicated(series_data$time)),]
if (length(which(duplicated(status_data$time)))!=0) status_data <- status_data[-which(duplicated(status_data$time)),]
if (select_data$otherTypeLoc == "GPS") {
  if (length(which(duplicated(gps_data$time)))!=0) gps_data <- gps_data[-which(duplicated(gps_data$time)),]
}

### Merge the latitude and longitude observations from Argos and GPS
## If time measurements exist at the Argo and the GPS source, the latitude and the longitude from the GPS source are selected
if (select_data$otherTypeLoc == "GPS") {
  mergeloc <- merge(loc_data,gps_data, all = T)
  colnames(mergeloc) <- c("tagid","time","lc","lat_argos","lon_argos","satellites","residual","lat_gps","lon_gps")
  mergeloc$lat <- NA
  mergeloc$lon <- NA
  mergeloc$source_loc <- name_file[1]
  dimmerge <- dim(mergeloc)[1]
    for (i in 1:dimmerge){
      if (length(which(mergeloc$time[i] == gps_data$time)) != 0) { 
        mergeloc[i,c("lat","lon")] <- mergeloc[i,c("lat_gps","lon_gps")]
        mergeloc[i,c("lc")] <- "gps"
        mergeloc[i,c("source_loc")] <- name_file[4] # Warning : be careful with the order
  
      } else {
        mergeloc[i,c("lat","lon")] <- mergeloc[i,c("lat_argos","lon_argos")]
      }
    }
  mergeloc <- mergeloc[,c("tagid","time","lc","satellites","residual","lat","lon","source_loc")]
} else { 
  mergeloc <- loc_data
  mergeloc$source_loc <- rep(name_file[1]) # Warning : be careful with the order
}

### Add a column to describe the source data type for Series and Status
series_data$source_series <- rep(name_file[3]) # Warning : be careful with the order
status_data$source_status <- rep(name_file[2]) # Warning : be careful with the order
  
#######################################################################################################

###################################### Step 3: Merge existing data ####################################


### Merge the observations with the same time - Works only if a variable is chosen only once from csv
## files (except time and tagid)
fusion <- merge(mergeloc,series_data, all=TRUE)
fusion <- merge(fusion,status_data, all=TRUE)

#######################################################################################################

###################################### Step 4: Data export in a csv file ####################################

output.file <- paste("data/output/",tag_id,"/",level,"/",tag_id,"-",level,".csv", sep= "")
write.table(fusion, output.file, row.names=FALSE, sep=",", dec=".")

#######################################################################################################

###################################### Step 5: Prepare the data for the NetCDF document ###############
## To add a new variable with attributes (long_name,units,details) in the NetCDF: modify the previous step and the CSV 
## file parameter_netcdf
## To add other attributes such as maximum, minimum, add them manualy (see also maximum and minimum for the temperature)
## The dimensions and the details for the NC_Global are inserted manually

### Import parameters from parameter_netcdf.csv
## WARNING the name of the data created in the previous step have to be the same as the var_name in 
## allparameter (or long_name for NC_GLOBAL).
allparameter <- "data/parameter_netcdf.csv"
allparameter <- read.csv(allparameter, sep=",", dec=".", header=TRUE, fill=TRUE)
if (select_data$otherTypeLoc != "GPS") { 
  allparameter <- allparameter[-which(allparameter$var_name == "satellites"),]
  allparameter <- allparameter[-which(allparameter$var_name == "residual"),]
}

product <- paste(level,"product",sep = "")

variables <- allparameter[which(allparameter[[product]] == "x"),]
for (i in 1:length(variables)) { variables[,i] <- as.character(variables[,i])}
dimvar <- dim(variables)[1]

glob_att<- "data/nc_global_att.csv"
glob_att <- read.csv(glob_att, sep=",", dec=".", header=TRUE, fill=TRUE)
glob_att <- glob_att[which(glob_att[[product]] == "x"),]
for (i in 1:length(glob_att)) { glob_att[,i] <- as.character(glob_att[,i])}
dimglob <- dim(glob_att)[1]

#######################################################################################################

###################################### Step 6: Creation of NetCDF ####################################

### Creation of new NetCDF file
data_type <- "netcdf"
filename <- paste("data/output/",tag_id,"/",level,"/",tag_id,"-",level,".nc", sep="")
dataset <- create.nc(filename)

### Creation of new NetCDF dimensions, the dimension is defined by the length of the variables
## A NetCDF file may only contain one unlim dimension. We choose the variable with the most observations (time)
dim.def.nc(dataset, "time", unlim=TRUE)
## For the variables with characters, there are 2 dimensions: (1) the number of observations (like "time")
## and (2) the maximum length of the string. This second dimension is created here:
dim.def.nc(dataset, "max_string_32", 32)
dim.def.nc(dataset, "max_string_4", 4)



### Creation of new NetCDF variables
## Definition of the variables in the NetCDF file with the format 
## var.def.nc(netcdf_file, "variable_name", "value_type","dimension"), such as: 
## - var.def.nc(dataset, "time", "NC_INT","time")
## - var.def.nc(dataset, "lat", "NC_DOUBLE","time")
## - var.def.nc(dataset, "source_loc", "NC_CHAR",c("max_string_32","time")): for the character, 
##   the UNLIM dimension has to be at last in the dimension vector
for ( i in 1:dimvar ) { 
  if (variables$value_type[i]=="NC_CHAR"){
    var.def.nc(dataset, variables$var_name[i], variables$value_type[i], c(variables$dimCHAR[i],variables$dimL0product[i]))
  } else {
    var.def.nc(dataset, variables$var_name[i], variables$value_type[i], variables$dimL0product[i]) 
  }
}
## To view the NetCDF file
# print.nc(dataset) # at this step the dimension of the time is NULL, because the observations are not added



### Put attributes in the variables or in the NC_GLOBAL
## The attributes are either the meta data of the variables or the global meta data of the NetCDF (NC_GLOBAL)
## the format is : att.put.nc(netcdf_file, "variables_name-or-NC_GLOBAL", "meta_data_name", "value_type", data), such as:
## - att.put.nc(dataset, "NC_GLOBAL", "title", "NC_CHAR", title)
## - att.put.nc(dataset, "time", "long_name" , "NC_CHAR", name_time)
## - att.put.nc(dataset, "temp", "_FillValue", "NC_DOUBLE", -99999.9), _FillValue has to be added for the creation of the figures
#
## For NC_GLOBAL
## WARNING the names of the data in select_data have to be the same as the colomn "att_name" in glob_att
for ( i in 1:dimglob ) { 
  if ( length(intersect(colnames(select_data),glob_att$att_name[i])) == 1)  {
    id.glob_att <- which(colnames(select_data) == glob_att$att_name[i]) 
    id.glob_att <- as.numeric(id.glob_att)
    att.put.nc(dataset, "NC_GLOBAL", glob_att$att_name[i], glob_att$value_type[i],  format(select_data[,id.glob_att]) ) # format is to keep the format of the select_data
  }
}
## Other attributes for NC_GLOBAL
detail_1 <- "L0 product : raw data."
att.put.nc(dataset, "NC_GLOBAL", "detail_1", "NC_CHAR", detail_1 )
if ( download_from_WCDataPortal == "yes"){
  DATE <- format(Sys.time(), "%d-%b-%Y %X ")
  detail_2 <- paste("Data from WC Data Portal (", DATE,").", sep="")
  att.put.nc(dataset, "NC_GLOBAL", "detail_2", "NC_CHAR", detail_2 )
}


#
## For variables 
for ( i in 1:dimvar ) { 
  if (variables$standard_name[i] != "") {
    att.put.nc(dataset, variables$var_name[i], "standard name", "NC_CHAR", variables$standard_name[i])  # add standard name
  }
  if (variables$long_name[i] != "") {
    att.put.nc(dataset, variables$var_name[i], "long_name", "NC_CHAR", variables$long_name[i])  # add a long name 
  } 
  if (variables$units[i] != "") {
    att.put.nc(dataset, variables$var_name[i], "units", "NC_CHAR", variables$units[i]) # add the unit for the variables having an unit 
  }
  if (variables$value_type[i] == "NC_DOUBLE"){
    att.put.nc(dataset, variables$var_name[i], "_FillValue", "NC_DOUBLE", -99999.9) # -99999.9 to see Michna, P. & Milton Woods. RNetCDF - A Package for Reading and Writing NetCDF Datasets. (2013).
  }
  if (variables$details[i] != "") {
    att.put.nc(dataset, variables$var_name[i], "details", "NC_CHAR", variables$details[i])  # add details
  }
}
## Other attributes
max_temp <- max(fusion$temp, na.rm = TRUE)
min_temp <- min(fusion$temp, na.rm = TRUE)
max_depth <- max(fusion$depth, na.rm = TRUE)
min_depth <- min(fusion$depth, na.rm = TRUE)
att.put.nc(dataset, "temp", "max" , "NC_DOUBLE", max_temp )
att.put.nc(dataset, "temp", "min" , "NC_DOUBLE", min_temp )
att.put.nc(dataset, "depth", "max" , "NC_DOUBLE", max_depth )
att.put.nc(dataset, "depth", "min" , "NC_DOUBLE", min_depth )


### Write the contents of a NetCDF variable.
## format: var.put.nc(netcdf_file, varialable_name, data), such as: var.put.nc(dataset, "lon", fusion$lon)
## the time variable data must be temporarily converted to a UTC referenced date, format of the convertion: dataconvert <- utinvcal.nc(units, data)
## for CHAR the NA must be replaced by ""
## Warning: the var.put.nc will not work if the format of the data is different from the format given in var.def.nc
for (i in 1 : dimvar){
  if (variables$var_name[i] =="time"){
    mytime <- utinvcal.nc(variables$units[which(variables$var_name == "time")], fusion$time) #conversion of time
    var.put.nc(dataset, "time", mytime)
  } else  if (variables$value_type[i] == "NC_CHAR"){
    id.char <- as.numeric(which(colnames(fusion) == variables$var_name[i])) # select the id of variables using character in fusion 
    mydata <- fusion[,id.char] # select the data
    mydata <- as.character(mydata) #  warning : the "as.character" have to be before ' remplace NA by "" '
    mydata[is.na(mydata)] <-"" # remplace NA by ""
    var.put.nc(dataset, variables$var_name[i], mydata) 
  } else {
    id.var <- as.numeric(which(colnames(fusion) == variables$var_name[i])) #select the other id
    var.put.nc(dataset, variables$var_name[i], fusion[,id.var])
  }
}

### View the NetCDF
# print.nc(dataset)
# var.get.nc(dataset, "source_loc")

### Close the opened NetCDF file
close.nc(dataset)
##########################################      END      ##############################################
